# 用法

```bash
docker search java
docker pull java
docker images
docker save docker.io/java > /root/java.tar.gz
docker rmi docker.io/java
docker ps -a # 查看容器运行状态
```

容器生命周期管理

```bash
docker [run|start|stop|restart|kill|rm|pause|unpause]
```

容器操作运维

```bash
docker [ps|inspect|top|attach|events|logs|wait|export|port]
```

容器rootfs命令


```bash
docker [commit|cp|diff]
```

镜像仓库

```bash
docker [login|pull|push|search]
```

本地镜像管理

```bash
docker [images|rmi|tag|build|history|save|import]
```

其他命令

```bash
docker [info|version]
```

## 启动

-it : 以交互式启动容器
-name : 指定容器别名

```bash
docker run -it --name myjava docker.io/java
```

## 映射

### 端口映射

-p 9001:8080

9000:宿主机9000端口

8085:docker 虚拟主机8085端口

把 docker 虚拟主机的8085端口映射到宿主机的9000端口上

### 文件映射

-v /root/project:/soft --privileged

/root/project : 宿主机 /root/project 目录
/soft : docker 虚拟主机 /soft 目录
--privileged : 给予最高权限

```bash
docker run -it --name myjava -p 9000:8080 -p 9001:8085 -v /root/project:/soft --privileged docker.io/java bash
```

## 暂停和停止

```bash
docker pause myjava #暂停
docker unpause myjava #恢复
docker stop myjava #停止
docker start -i myjava #启动
```

## docker 创建 pxc 集群

### 拉取 pxc 镜像 

```bash
docker pull docker.io/percona/percona-xtradb-cluster
docker tag docker.io/percona/percona-xtradb-cluster pxc # 重命名一个简短名字
docker rmi docker.io/percona/percona-xtradb-cluster
```

### 给 pxc 集群创建 docker 内部网络

```bash
dcoker network create net1 # 创建网络段 net1
docker network inspect net1 # 查看 net1 网络段信息
docker network rm net1 # 删除网络段 net1
```

### 给 pxc 集群创建 docker 卷 

```bash
docker volume create --name v1 # 创建 docker 卷 并命名为 v1
docker inspect v1 # 查看 v1 卷
docker volume rm v1 # 删除 v1 卷
```

### 创建 pxc 容器

```bash
docker run -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=abc123456 -e CLUSTER_NAME=PXC -e XTRABACKUP_PASSWORD=abc123456 -v v1:/var/lib/mysql --privileged --name=node1 --net=net1 --ip 172.18.0.2 pxc
```

### 创建 pxc 容器集群

以 node1 为基础

```bash
docker run -d -p 3307:3306 -e MYSQL_ROOT_PASSWORD=abc123456 -e CLUSTER_NAME=PXC -e XTRABACKUP_PASSWORD=abc123456 -e CLUSTER_JOIN=node1 -v v2:/var/lib/mysql --privileged --name=node2 --net=net1 --ip 172.18.0.3 pxc
docker run -d -p 3308:3306 -e MYSQL_ROOT_PASSWORD=abc123456 -e CLUSTER_NAME=PXC -e XTRABACKUP_PASSWORD=abc123456 -e CLUSTER_JOIN=node1 -v v3:/var/lib/mysql --privileged --name=node3 --net=net1 --ip 172.18.0.4 pxc
docker run -d -p 3309:3306 -e MYSQL_ROOT_PASSWORD=abc123456 -e CLUSTER_NAME=PXC -e XTRABACKUP_PASSWORD=abc123456 -e CLUSTER_JOIN=node1 -v v4:/var/lib/mysql --privileged --name=node4 --net=net1 --ip 172.18.0.5 pxc
docker run -d -p 3310:3306 -e MYSQL_ROOT_PASSWORD=abc123456 -e CLUSTER_NAME=PXC -e XTRABACKUP_PASSWORD=abc123456 -e CLUSTER_JOIN=node1 -v v5:/var/lib/mysql --privileged --name=node5 --net=net1 --ip 172.18.0.6 pxc
```

	CLUSTER_JOIN=node1 : 加入 node1节点.
	创建节点时需等前一个节点mysql 初始化完成.

## 数据库负载均衡

### 拉取haproxy镜像

```bash
docker pull haproxy
```

### 创建 haproxy 配置文件

```bash
mkdir /home/soft/haproxy
vim /home/soft/haproxy/haproxy.cfg

defaults
        log global
        mode    http
        option  httplog
        option  dontlognull
        timeout connect 5000
        timeout client  50000
        timeout server  50000
listen  admin_stats
        bind    0.0.0.0:8888
        mode            http
        stats uri       /dbs
        stats realm     Global\ statistics
        stats auth      admin:abc123456
listen  proxy-mysql
        bind    0.0.0.0:3306
        mode tcp
        balance roundrobin
        option  tcplog
        option  mysql-check user haproxy
        option  MySQL_1 172.18.0.2:3306 check weight 1 maxconn 2000
        option  MySQL_2 172.18.0.3:3306 check weight 1 maxconn 2000
        option  MySQL_3 172.18.0.4:3306 check weight 1 maxconn 2000
        option  MySQL_4 172.18.0.5:3306 check weight 1 maxconn 2000
        option  MySQL_5 172.18.0.6:3306 check weight 1 maxconn 2000
        option tcpka
```

### 创建 haproxy 容器

```bash
docker run -it -d -p 4001:8888 -p 4002:3306 -v /home/soft/haproxy:/usr/local/etc/haproxy --name haproxy1 --privileged --net=net1 --ip 172.18.0.7 haproxy
```

进入 haproxy1 容器,指定haproxy 启动配置指令

```bash
docker exec -it haproxy1 bash
haproxy -f /usr/local/etc/haproxy/haproxy.cfg
```

创建haproxy 的 mysql 用户

```bash
mysql -h 172.18.0.2 -uroot -p
```

```mysql
create user 'haproxy'@'%' identified by '';
```

### haproxy 监控:

[http://ip:4001/dbs](https://ip:4001/dbs)

## 利用 keepalived 实现双机热备

### 安装 keepalived

进入 haproxy 容器,更新源后安装 keepalived

```bash
docker exec -it haproxy1 bash
apt-get update
apt-get install -y keepalived
```

### 配置 keepalived

```bash
vim /etc/keepalived/keepalived.conf

vrrp_instance  VI_1 {
    state  MASTER
    interface  eth0
    virtual_router_id  51
    priority  100
    advert_int  1
    authentication {
        auth_type  PASS
        auth_pass  123456
    }
    virtual_ipaddress {
        172.18.0.201
    }
}
```

启动 keepalived

```bash
service keepalived start
```

在宿主机里测试keepalived是否启动

```bash
ping 172.18.0.201
```

### 宿主机安装配置 keepalived

```bash
yum install -y keepalived

ip addr

vim /etc/keepalived/keepalived.conf

```

keepalived 配置参数:
```
vrrp_instance VI_1 {
    state MASTER
    interface ens33
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
       	192.168.41.170
       	# 指定一个虚拟 ip
    }
}


virtual_server 192.168.41.170 8888 {
    delay_loop 3
    lb_algo rr 
    lb_kind NAT
    persistence_timeout 50
    protocol TCP

    real_server 172.18.0.201 8888 {
        weight 1
    }
}


virtual_server 192.168.41.170 3306 {
    delay_loop 3
    lb_algo rr 
    lb_kind NAT
    persistence_timeout 50
    protocol TCP

    real_server 172.18.0.201 3306 {
        weight 1
    }
}

```

重启 keepalived

```bash
systemctl restart keepalived
systemctl enable keepalived	
```


直接通过docker start node1 或者任何一个节点是启动不了的，原因是集群之前的同步机制造成的，启动任何一个节点，该节点都会去其它节点同步数据，其它节点仍处于宕机状态，所以该节点启动失败，这也是pxc集群的强一致性的表现，解决方式是，删除所有节点docker rm node1 node2 node3 node4 node 5

和数据卷中的grastate.dat文件

rm -rf /var/lib/docker/volumes/v1/_data/grastate.dat
rm -rf /var/lib/docker/volumes/v2/_data/grastate.dat
rm -rf /var/lib/docker/volumes/v3/_data/grastate.dat
rm -rf /var/lib/docker/volumes/v4/_data/grastate.dat
rm -rf /var/lib/docker/volumes/v5/_data/grastate.dat
重新执行集群创建的命令即可，因为数据都在数据卷中，所有放心，集群重新启动都数据仍然都在.
